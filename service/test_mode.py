"""
Module de test avec √©chantillonnage al√©atoire et logging d√©taill√©
"""

import pandas as pd
import random
import json
import os
from datetime import datetime
import sys

# Ajouter le r√©pertoire parent au path pour les imports
sys.path.append(os.path.join(os.path.dirname(__file__), '..'))


class TestLogger:
    """
    Logger d√©taill√© pour le mode test
    """
    
    def __init__(self, log_file_path=None):
        if log_file_path is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            log_file_path = f"test_log_{timestamp}.json"
        
        self.log_file_path = log_file_path
        self.logs = []
        self.current_establishment = None
    
    def start_establishment(self, idx, establishment_name, city, department):
        """
        D√©marre le logging pour un nouvel √©tablissement
        """
        self.current_establishment = {
            "index": idx,
            "establishment_name": establishment_name,
            "city": city,
            "department": department,
            "timestamp": datetime.now().isoformat(),
            "steps": [],
            "final_result": None
        }
        print(f"\nüîç [TEST LOG] D√©but traitement √©tablissement #{idx}: {establishment_name}")
    
    def log_step(self, step_name, details, data=None):
        """
        Enregistre une √©tape du processus
        """
        step = {
            "step": step_name,
            "timestamp": datetime.now().isoformat(),
            "details": details,
            "data": data
        }
        
        if self.current_establishment:
            self.current_establishment["steps"].append(step)
        
        print(f"üìù [TEST LOG] {step_name}: {details}")
        if data and isinstance(data, dict):
            for key, value in data.items():
                print(f"    ‚Ä¢ {key}: {value}")
    
    def log_geo_strategy(self, geo_strategy, search_city, search_dept):
        """
        Log de la strat√©gie g√©ographique
        """
        self.log_step(
            "Strat√©gie g√©ographique",
            f"Type: {geo_strategy['type']}, Ville: {geo_strategy['use_city']}, D√©partement: {geo_strategy['use_department']}",
            {
                "search_city_normalized": search_city,
                "search_department_normalized": search_dept,
                "strategy": geo_strategy
            }
        )
    
    def log_candidates_found(self, candidates_count, candidates_list):
        """
        Log des candidats trouv√©s
        """
        candidates_details = []
        for i, (name, finess) in enumerate(candidates_list, 1):
            candidates_details.append({
                "index": i,
                "name": name,
                "finess": finess
            })
        
        self.log_step(
            "Candidats trouv√©s",
            f"{candidates_count} candidats identifi√©s",
            {"candidates": candidates_details}
        )
    
    def log_fuzzy_matching(self, establishment_name_clean, best_score, best_match_idx, threshold):
        """
        Log du matching fuzzy
        """
        self.log_step(
            "Matching Fuzzy",
            f"Meilleur score: {best_score}% (seuil: {threshold}%)",
            {
                "establishment_clean": establishment_name_clean,
                "best_score": best_score,
                "best_match_index": best_match_idx,
                "threshold": threshold,
                "fuzzy_success": best_score > threshold
            }
        )
    
    def log_ai_prompt(self, prompt, candidates_list, establishment_type):
        """
        Log du prompt envoy√© √† l'IA
        """
        self.log_step(
            "Prompt IA envoy√©",
            f"Analyse de {len(candidates_list)} candidats par IA",
            {
                "establishment_type": establishment_type,
                "candidates_count": len(candidates_list),
                "full_prompt": prompt,
                "candidates_analyzed": [{"name": name, "finess": finess} for name, finess in candidates_list]
            }
        )
    
    def log_ai_response(self, raw_response, selected_index, matched_name, confidence):
        """
        Log de la r√©ponse de l'IA
        """
        self.log_step(
            "R√©ponse IA re√ßue",
            f"S√©lection: index {selected_index}, confiance: {confidence}%",
            {
                "raw_ai_response": raw_response,
                "selected_index": selected_index,
                "matched_name": matched_name,
                "confidence_score": confidence
            }
        )
    
    def log_final_result(self, finess, matched_name, confidence, method_used):
        """
        Log du r√©sultat final
        """
        self.current_establishment["final_result"] = {
            "finess": finess,
            "matched_name": matched_name,
            "confidence": confidence,
            "method_used": method_used
        }
        
        self.log_step(
            "R√©sultat final",
            f"FINESS: {finess}, M√©thode: {method_used}",
            {
                "finess": finess,
                "matched_name": matched_name,
                "confidence": confidence,
                "method": method_used
            }
        )
    
    def finish_establishment(self):
        """
        Termine le logging pour l'√©tablissement actuel
        """
        if self.current_establishment:
            self.logs.append(self.current_establishment)
            print(f"‚úÖ [TEST LOG] Fin traitement √©tablissement #{self.current_establishment['index']}")
            self.current_establishment = None
    
    def save_logs(self):
        """
        Sauvegarde tous les logs dans un fichier JSON
        """
        try:
            with open(self.log_file_path, 'w', encoding='utf-8') as f:
                json.dump(self.logs, f, indent=2, ensure_ascii=False)
            print(f"üíæ [TEST LOG] Logs sauvegard√©s dans: {self.log_file_path}")
            return self.log_file_path
        except Exception as e:
            print(f"‚ùå [TEST LOG] Erreur sauvegarde logs: {e}")
            return None
    
    def generate_summary(self):
        """
        G√©n√®re un r√©sum√© des tests
        """
        if not self.logs:
            return "Aucun log disponible"
        
        total = len(self.logs)
        successful = sum(1 for log in self.logs if log["final_result"] and log["final_result"]["finess"])
        
        methods_used = {}
        for log in self.logs:
            if log["final_result"]:
                method = log["final_result"].get("method_used", "unknown")
                methods_used[method] = methods_used.get(method, 0) + 1
        
        summary = f"""
üéØ === R√âSUM√â DU TEST ===
üìä Total √©tablissements test√©s: {total}
‚úÖ Matches trouv√©s: {successful} ({successful/total*100:.1f}%)
‚ùå Aucun match: {total - successful} ({(total-successful)/total*100:.1f}%)

üìà M√©thodes utilis√©es:
"""
        for method, count in methods_used.items():
            summary += f"   ‚Ä¢ {method}: {count} cas ({count/total*100:.1f}%)\n"
        
        summary += f"\nüìÅ Log d√©taill√©: {self.log_file_path}"
        
        return summary


class TestSampler:
    """
    Gestionnaire d'√©chantillonnage pour les tests
    """
    
    def __init__(self, sample_size=25, random_seed=None):
        self.sample_size = sample_size
        self.random_seed = random_seed
        if random_seed:
            random.seed(random_seed)
    
    def sample_dataframe(self, df, sample_size=None):
        """
        √âchantillonne al√©atoirement un DataFrame
        """
        if sample_size is None:
            sample_size = self.sample_size
        
        if len(df) <= sample_size:
            print(f"‚ö†Ô∏è  DataFrame contient seulement {len(df)} lignes, utilisation compl√®te")
            return df.copy()
        
        # √âchantillonnage al√©atoire
        sampled_indices = random.sample(range(len(df)), sample_size)
        sampled_df = df.iloc[sampled_indices].copy().reset_index(drop=True)
        
        print(f"üé≤ √âchantillon de {sample_size} √©tablissements s√©lectionn√© sur {len(df)} total")
        print(f"üìã Indices s√©lectionn√©s: {sorted(sampled_indices)}")
        
        return sampled_df
    
    def create_test_config(self, original_df, sampled_df):
        """
        Cr√©e une configuration de test
        """
        config = {
            "test_timestamp": datetime.now().isoformat(),
            "original_size": len(original_df),
            "sample_size": len(sampled_df),
            "random_seed": self.random_seed,
            "sample_info": {
                "first_establishment": sampled_df.iloc[0].to_dict() if len(sampled_df) > 0 else None,
                "last_establishment": sampled_df.iloc[-1].to_dict() if len(sampled_df) > 0 else None
            }
        }
        return config


def create_test_hospital_matcher(original_matcher, enable_test_mode=True, sample_size=25):
    """
    Cr√©e une version test du HospitalMatcher avec √©chantillonnage et logging
    """
    if not enable_test_mode:
        return original_matcher
    
    class TestHospitalMatcher(original_matcher.__class__):
        """
        Version test du HospitalMatcher avec logging d√©taill√©
        """
        
        def __init__(self, *args, **kwargs):
            super().__init__(*args, **kwargs)
            self.test_mode = True
            self.test_logger = TestLogger()
            self.test_sampler = TestSampler(sample_size=sample_size)
            
            # Cr√©er un chemin de sortie sp√©cifique pour les tests
            self._setup_test_output_path()
            
        def _setup_test_output_path(self):
            """
            Configure un chemin de sortie sp√©cifique pour les tests
            """
            from config import OUTPUT_PATH
            import os
            
            # G√©n√©rer un nom de fichier unique pour le test
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            base_dir = os.path.dirname(OUTPUT_PATH)
            base_name = os.path.splitext(os.path.basename(OUTPUT_PATH))[0]
            
            # Nom sp√©cifique pour le mode test
            self.test_output_path = os.path.join(
                base_dir, 
                f"{base_name}_TEST_SAMPLE_{sample_size}_{timestamp}.xlsx"
            )
            
            print(f"üìÅ [TEST] Fichier de sortie: {self.test_output_path}")
            
        def load_data(self):
            """
            Charge les donn√©es et applique l'√©chantillonnage en mode test
            """
            print("\nüß™ === MODE TEST ACTIV√â ===")
            super().load_data()
            
            # √âchantillonner les donn√©es
            print(f"üé≤ √âchantillonnage de {sample_size} √©tablissements...")
            original_df = self.df_lp.copy()  # Sauvegarder l'original pour la config
            self.df_lp = self.test_sampler.sample_dataframe(self.df_lp, sample_size)
            
            # Cr√©er la configuration de test
            test_config = self.test_sampler.create_test_config(original_df, self.df_lp)
            self.test_logger.log_step("Configuration test", "√âchantillonnage effectu√©", test_config)
            
            # Sauvegarder les informations d'√©chantillonnage dans un fichier s√©par√©
            self._save_sample_info(test_config, original_df)
            
        def _save_sample_info(self, test_config, original_df):
            """
            Sauvegarde les informations sur l'√©chantillon dans un fichier Excel s√©par√©
            """
            try:
                sample_info_path = self.test_output_path.replace('.xlsx', '_SAMPLE_INFO.xlsx')
                
                # Cr√©er un DataFrame avec les informations de l'√©chantillon
                sample_df = self.df_lp.copy()
                sample_df['ORIGINAL_INDEX'] = sample_df.index
                sample_df['SELECTED_FOR_TEST'] = True
                
                # Cr√©er un DataFrame r√©sum√©
                summary_data = {
                    'Param√®tre': [
                        'Taille originale',
                        'Taille √©chantillon', 
                        'Pourcentage √©chantillonn√©',
                        'Graine al√©atoire',
                        'Timestamp test',
                        'Fichier de logs',
                        'Fichier r√©sultats'
                    ],
                    'Valeur': [
                        test_config['original_size'],
                        test_config['sample_size'],
                        f"{(test_config['sample_size']/test_config['original_size']*100):.1f}%",
                        test_config['random_seed'] or 'Al√©atoire',
                        test_config['test_timestamp'],
                        self.test_logger.log_file_path,
                        self.test_output_path
                    ]
                }
                summary_df = pd.DataFrame(summary_data)
                
                # Sauvegarder dans un fichier Excel avec plusieurs feuilles
                with pd.ExcelWriter(sample_info_path, engine='openpyxl') as writer:
                    summary_df.to_excel(writer, sheet_name='R√©sum√© Test', index=False)
                    sample_df.to_excel(writer, sheet_name='√âchantillon S√©lectionn√©', index=False)
                
                print(f"üìã [TEST] Informations √©chantillon sauvegard√©es: {sample_info_path}")
                
            except Exception as e:
                print(f"‚ö†Ô∏è  [TEST] Erreur sauvegarde info √©chantillon: {e}")

        def _process_single_hospital(self, idx, row):
            """
            Version avec logging d√©taill√©
            """
            from config import COLA_NOM_HOPITAL, COLA_VILLE, COLA_DEPARTEMENT, COLA_FINESS
            from establishment_utils import get_establishment_name_and_type
            
            # Commencer le logging pour cet √©tablissement
            establishment_name, _ = get_establishment_name_and_type(row)
            city = row.get(COLA_VILLE, "N/A")
            dept = row.get(COLA_DEPARTEMENT, "N/A")
            
            self.test_logger.start_establishment(idx, establishment_name, city, dept)
            
            # Appeler la m√©thode parent avec logging
            try:
                result = super()._process_single_hospital(idx, row)
                
                # Logger le r√©sultat
                finess = self.df_lp.at[idx, COLA_FINESS] if COLA_FINESS in self.df_lp.columns else None
                matched_name = self.df_lp.at[idx, "Nom_Match_Retenu"] if "Nom_Match_Retenu" in self.df_lp.columns else None
                confidence = self.df_lp.at[idx, "Confiance_Match"] if "Confiance_Match" in self.df_lp.columns else 0
                
                method = "unknown"
                if finess:
                    method = "fuzzy" if confidence >= 80 else "ai"
                else:
                    method = "no_match"
                
                self.test_logger.log_final_result(finess, matched_name, confidence, method)
                
            except Exception as e:
                self.test_logger.log_step("Erreur", f"Erreur durant le traitement: {str(e)}")
                raise
            finally:
                self.test_logger.finish_establishment()
            
            return result
        
        def _find_candidates_in_city(self, row, establishment_name, establishment_type):
            """
            Version avec logging de la recherche g√©ographique
            """
            from config import get_geo_comparison_strategy, COLA_VILLE, COLA_DEPARTEMENT, COLB_VILLE, COLB_DEPARTEMENT
            from geo_utils import normalize_city_name, normalize_department_name
            
            # Logger la strat√©gie g√©ographique
            geo_strategy = get_geo_comparison_strategy()
            
            search_city = None
            search_dept = None
            
            if geo_strategy["use_city"] and COLA_VILLE and COLA_VILLE in row.index:
                search_city = normalize_city_name(row[COLA_VILLE])
            
            if geo_strategy["use_department"] and COLA_DEPARTEMENT and COLA_DEPARTEMENT in row.index:
                search_dept = normalize_department_name(row[COLA_DEPARTEMENT])
            
            self.test_logger.log_geo_strategy(geo_strategy, search_city, search_dept)
            
            # Appeler la m√©thode parent
            candidates = super()._find_candidates_in_city(row, establishment_name, establishment_type)
            
            # Logger les candidats trouv√©s
            self.test_logger.log_candidates_found(len(candidates), candidates)
            
            return candidates
        
        def _match_establishment(self, establishment_name, establishment_type, candidates):
            """
            Version avec logging du matching
            """
            from config import FUZZY_THRESHOLD
            from establishment_utils import clean_name
            
            establishment_name_clean = clean_name(establishment_name)
            
            # Si un seul candidat
            if len(candidates) == 1:
                self.test_logger.log_step("Match unique", f"Un seul candidat: {candidates[0][0]}")
                return candidates[0][1], candidates[0][0], 95, "COLB_NOM_SC"
            
            # Essayer le fuzzy matching avec logging
            best_match_idx, best_score = self._fuzzy_match(establishment_name_clean, candidates)
            self.test_logger.log_fuzzy_matching(establishment_name_clean, best_score, best_match_idx, FUZZY_THRESHOLD)
            
            if best_score > FUZZY_THRESHOLD:
                method = "fuzzy_match"
                result = candidates[best_match_idx][1], candidates[best_match_idx][0], best_score, "COLB_NOM_SC"
            else:
                # Utiliser l'IA avec logging du prompt
                method = "ai_match"
                result = self._ai_match_with_logging(establishment_name_clean, candidates, establishment_type)
                # Ajouter la source_column si pas pr√©sente
                if len(result) == 3:
                    result = result + ("COLB_NOM_SC",)
            
            # Logger selon la m√©thode utilis√©e
            finess, matched_name, confidence, source_column = result
            if finess:
                self.test_logger.log_final_result(finess, matched_name, confidence, method)
            else:
                self.test_logger.log_final_result(None, "AUCUNE CORRESPONDANCE", 0, "no_match")
            
            return result
        
        def _ai_match_with_logging(self, establishment_name_clean, candidates, establishment_type):
            """
            Matching IA avec logging d√©taill√© du prompt et de la r√©ponse
            """
            from ai_service import ai_compare_hospital_names_batch
            from establishment_utils import detect_establishment_type
            
            # Construire le prompt exactement comme dans ai_service.py (SANS les types)
            candidates_text = ""
            for i, (name, finess) in enumerate(candidates, 1):
                # Supprimer l'affichage du type d√©tect√© - ne pas le montrer √† Gemini
                candidates_text += f"- Option {i}: \"{name}\" (FINESS: {finess})\n"
            
            establishment_info = self._get_establishment_info(establishment_type)
            
            prompt = f"""
Tu es un expert en analyse de donn√©es m√©dicales. Tu dois OBLIGATOIREMENT choisir la meilleure correspondance parmi les options donn√©es.

Nom de l'√©tablissement recherch√©: "{establishment_name_clean}"

{establishment_info}

Options disponibles:
{candidates_text}

R√®gles STRICTES:
- Tu DOIS choisir une option, m√™me si la correspondance n'est pas parfaite
- Si aucun √©tablissement du m√™me type, choisis le plus proche par nom
- Ignore les diff√©rences mineures (accents, espaces, tirets, ponctuation)
- Les abr√©viations sont accept√©es (ex: "CARDIO" pour "CARDIOLOGIE")
- En cas de doute entre √©tablissements du m√™me type, choisis le nom le plus d√©taill√©
- les libelles comporte aussi la ville et le d√©partement, ne les prends pas en compte dans la comparaison
Cependant si la correspondance est vraiment pas pareil, tu peux retourner 0 pour indiquer "AUCUNE CORRESPONDANCE".

R√©ponds uniquement par le num√©ro de l'option choisie (1, 2, 3, etc.)
"""
            
            # Logger le prompt
            self.test_logger.log_ai_prompt(prompt, candidates, establishment_type)
            
            # Appeler l'IA
            self.total_ai_requests += 1
            selected_idx, matched_name, confidence_score = ai_compare_hospital_names_batch(
                establishment_name_clean, candidates, establishment_type
            )
            
            # Logger la r√©ponse (on ne peut pas capturer la r√©ponse brute ici, mais on peut logger le r√©sultat)
            self.test_logger.log_ai_response(
                f"Index s√©lectionn√©: {selected_idx}", 
                selected_idx, 
                matched_name, 
                confidence_score
            )
            
            if selected_idx == -1:
                return None, "AUCUNE CORRESPONDANCE", 0, None
            
            return candidates[selected_idx][1], matched_name, confidence_score, "COLB_NOM_SC"

        def _get_establishment_info(self, establishment_type):
            """
            Helper pour obtenir les infos sur le type d'√©tablissement
            """
            if establishment_type == "hopital":
                return "Il s'agit d'un H√îPITAL, privil√©gie les √©tablissements de type h√¥pital (CHU, CH, HOPITAL, etc.)."
            elif establishment_type == "clinique":
                return "Il s'agit d'une CLINIQUE, privil√©gie les √©tablissements de type clinique (CLINIQUE, POLYCLINIQUE, etc.)."
            else:
                return "Type d'√©tablissement non d√©termin√©, utilise ton meilleur jugement."
        
        def save_results(self):
            """
            Sauvegarde avec g√©n√©ration du rapport de test et fichier Excel sp√©cifique
            """
            print(f"\nüíæ [TEST] Enregistrement des r√©sultats de test...")
            
            try:
                import os
                
                # Cr√©er le r√©pertoire si n√©cessaire
                test_dir = os.path.dirname(self.test_output_path)
                if not os.path.exists(test_dir):
                    os.makedirs(test_dir)
                
                # Ajouter des colonnes sp√©cifiques au test
                self.df_lp['TEST_MODE'] = True
                self.df_lp['TEST_TIMESTAMP'] = datetime.now().isoformat()
                self.df_lp['SAMPLE_SIZE'] = len(self.df_lp)
                
                # Sauvegarder dans le fichier sp√©cifique au test
                self.df_lp.to_excel(self.test_output_path, index=False)
                print(f"‚úÖ [TEST] R√©sultats test sauvegard√©s: {self.test_output_path}")
                
                # Sauvegarder les logs
                log_file = self.test_logger.save_logs()
                
                # G√©n√©rer et afficher le r√©sum√©
                summary = self.test_logger.generate_summary()
                print(summary)
                
                # Sauvegarder le r√©sum√© dans un fichier texte
                if log_file:
                    summary_file = log_file.replace('.json', '_summary.txt')
                    try:
                        with open(summary_file, 'w', encoding='utf-8') as f:
                            f.write(summary)
                        print(f"üìä [TEST] R√©sum√© sauvegard√©: {summary_file}")
                    except Exception as e:
                        print(f"‚ùå [TEST] Erreur sauvegarde r√©sum√©: {e}")
                
                # Cr√©er un fichier Excel consolid√© avec tous les r√©sultats
                self._create_consolidated_test_report(log_file)
                
            except Exception as e:
                print(f"‚ùå [TEST] Erreur lors de l'enregistrement: {e}")
                raise
        
        def _create_consolidated_test_report(self, log_file):
            """
            Cr√©e un rapport Excel consolid√© avec toutes les informations de test
            """
            try:
                consolidated_path = self.test_output_path.replace('.xlsx', '_RAPPORT_COMPLET.xlsx')
                
                # Pr√©parer les donn√©es pour le rapport
                test_results = self.df_lp.copy()
                
                # Statistiques g√©n√©rales
                total_tested = len(test_results)
                successful_matches = test_results[test_results['FINESS'].notna()].shape[0]
                success_rate = (successful_matches / total_tested * 100) if total_tested > 0 else 0
                
                stats_data = {
                    'M√©trique': [
                        'Total √©tablissements test√©s',
                        'Matches r√©ussis',
                        '√âchecs',
                        'Taux de r√©ussite (%)',
                        'Requ√™tes IA utilis√©es',
                        'Fichier de logs d√©taill√©',
                        'Timestamp du test'
                    ],
                    'Valeur': [
                        total_tested,
                        successful_matches,
                        total_tested - successful_matches,
                        f"{success_rate:.1f}%",
                        self.total_ai_requests,
                        log_file or 'N/A',
                        datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                    ]
                }
                stats_df = pd.DataFrame(stats_data)
                
                # Analyse par confiance
                if 'Confiance_Match' in test_results.columns:
                    confidence_ranges = {
                        'Tr√®s √©lev√©e (90-100%)': len(test_results[test_results['Confiance_Match'] >= 90]),
                        '√âlev√©e (80-89%)': len(test_results[(test_results['Confiance_Match'] >= 80) & (test_results['Confiance_Match'] < 90)]),
                        'Moyenne (70-79%)': len(test_results[(test_results['Confiance_Match'] >= 70) & (test_results['Confiance_Match'] < 80)]),
                        'Faible (60-69%)': len(test_results[(test_results['Confiance_Match'] >= 60) & (test_results['Confiance_Match'] < 70)]),
                        'Tr√®s faible (<60%)': len(test_results[test_results['Confiance_Match'] < 60])
                    }
                    
                    confidence_df = pd.DataFrame([
                        {'Niveau de confiance': k, 'Nombre': v, 'Pourcentage': f"{(v/total_tested*100):.1f}%"} 
                        for k, v in confidence_ranges.items()
                    ])
                else:
                    confidence_df = pd.DataFrame({'Info': ['Donn√©es de confiance non disponibles']})
                
                # Sauvegarder le rapport consolid√©
                with pd.ExcelWriter(consolidated_path, engine='openpyxl') as writer:
                    stats_df.to_excel(writer, sheet_name='Statistiques G√©n√©rales', index=False)
                    confidence_df.to_excel(writer, sheet_name='Analyse Confiance', index=False)
                    test_results.to_excel(writer, sheet_name='R√©sultats D√©taill√©s', index=False)
                
                print(f"üìà [TEST] Rapport consolid√© cr√©√©: {consolidated_path}")
                
            except Exception as e:
                print(f"‚ö†Ô∏è  [TEST] Erreur cr√©ation rapport consolid√©: {e}")

    # Copier les attributs de l'instance originale
    test_matcher = TestHospitalMatcher(
        reset_history=original_matcher.reset_history,
        differentiate_types=original_matcher.differentiate_types,
        forced_type=original_matcher.forced_type
    )
    
    return test_matcher
    return test_matcher
